{
    "docs": [
        {
            "location": "/", 
            "text": "This portal provides a detailled documentation of the OpenNMT toolkit. It describes how to use the PyTorch project and how it works.\n\n\nInstallation\n\n\n1. \nInstall PyTorch\n\n\n2. Clone the OpenNMT-py repository:\n\n\ngit clone https://github.com/OpenNMT/OpenNMT-py\n\ncd\n OpenNMT-py\n\n\n\n\n\n3. Install required libraries\n\n\npip install -r requirements.txt\n\n\n\n\n\nAnd you are ready to go! Take a look at the \nquickstart\n to familiarize yourself with the main training workflow.\n\n\nAlternatively you can use Docker to install with \nnvidia-docker\n. The main Dockerfile is included\nin the root directory.\n\n\nCitation\n\n\nWhen using OpenNMT for research please cite our\n\nOpenNMT technical report\n\n\n@inproceedings{opennmt,\n  author    = {Guillaume Klein and\n               Yoon Kim and\n               Yuntian Deng and\n               Jean Senellart and\n               Alexander M. Rush},\n  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},\n  booktitle = {Proc. ACL},\n  year      = {2017},\n  url       = {https://doi.org/10.18653/v1/P17-4012},\n  doi       = {10.18653/v1/P17-4012}\n}\n\n\n\n\n\nAdditional resources\n\n\nYou can find additional help or tutorials in the following resources:\n\n\n\n\nGitter channel", 
            "title": "Overview"
        }, 
        {
            "location": "/#installation", 
            "text": "1.  Install PyTorch  2. Clone the OpenNMT-py repository:  git clone https://github.com/OpenNMT/OpenNMT-py cd  OpenNMT-py  3. Install required libraries  pip install -r requirements.txt  And you are ready to go! Take a look at the  quickstart  to familiarize yourself with the main training workflow.  Alternatively you can use Docker to install with  nvidia-docker . The main Dockerfile is included\nin the root directory.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "When using OpenNMT for research please cite our OpenNMT technical report  @inproceedings{opennmt,\n  author    = {Guillaume Klein and\n               Yoon Kim and\n               Yuntian Deng and\n               Jean Senellart and\n               Alexander M. Rush},\n  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},\n  booktitle = {Proc. ACL},\n  year      = {2017},\n  url       = {https://doi.org/10.18653/v1/P17-4012},\n  doi       = {10.18653/v1/P17-4012}\n}", 
            "title": "Citation"
        }, 
        {
            "location": "/#additional-resources", 
            "text": "You can find additional help or tutorials in the following resources:   Gitter channel", 
            "title": "Additional resources"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Step 1: Preprocess the data\n\n\npython preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\n\n\n\n\n\nWe will be working with some example data in \ndata/\n folder.\n\n\nThe data consists of parallel source (\nsrc\n) and target (\ntgt\n) data containing one sentence per line with tokens separated by a space:\n\n\n\n\nsrc-train.txt\n\n\ntgt-train.txt\n\n\nsrc-val.txt\n\n\ntgt-val.txt\n\n\n\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\n\n\n$ head -n 3 data/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament \napos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .\n\nquot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\n\n\n\n\n\nStep 2: Train the model\n\n\npython train.py -data data/demo.train.pt -save_model demo-model \n\n\n\n\n\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add \n-gpuid 1\n to use (say) GPU 1.\n\n\nStep 3: Translate\n\n\npython translate.py -model demo-model_epochX_PPL.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose\n\n\n\n\n\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into \npred.txt\n.\n\n\n\n\nNote\n\n\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for \ntranslation\n or \nsummarization\n.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/quickstart/#step-1-preprocess-the-data", 
            "text": "python preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo  We will be working with some example data in  data/  folder.  The data consists of parallel source ( src ) and target ( tgt ) data containing one sentence per line with tokens separated by a space:   src-train.txt  tgt-train.txt  src-val.txt  tgt-val.txt   Validation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.  $ head -n 3 data/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament  apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym . quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .", 
            "title": "Step 1: Preprocess the data"
        }, 
        {
            "location": "/quickstart/#step-2-train-the-model", 
            "text": "python train.py -data data/demo.train.pt -save_model demo-model   The main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add  -gpuid 1  to use (say) GPU 1.", 
            "title": "Step 2: Train the model"
        }, 
        {
            "location": "/quickstart/#step-3-translate", 
            "text": "python translate.py -model demo-model_epochX_PPL.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose  Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into  pred.txt .   Note  The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for  translation  or  summarization .", 
            "title": "Step 3: Translate"
        }, 
        {
            "location": "/extended/", 
            "text": "The example below uses the Moses tokenizer (http://www.statmt.org/moses/) to prepare the data and the moses BLEU script for evaluation. This example if for training for the WMT'16 Multimodal Translation task (http://www.statmt.org/wmt16/multimodal-task.html).\n\n\n0) Download the data.\n\n\nmkdir -p data/multi30k\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz \n  tar -xf training.tar.gz -C data/multi30k \n rm training.tar.gz\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz \n tar -xf validation.tar.gz -C data/multi30k \n rm validation.tar.gz\nwget http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz \n tar -xf mmt_task1_test2016.tar.gz -C data/multi30k \n rm mmt_task1_test2016.tar.gz\n\n\n\n\n\n1) Preprocess the data.\n\n\n# Delete the last line of val and training files.\n\n\nfor\n l in en de\n;\n \ndo\n \nfor\n f in data/multi30k/*.\n$l\n;\n \ndo\n \nif\n \n[[\n \n$f\n !\n=\n *\ntest\n* \n]]\n;\n \nthen\n sed -i \n$\n d\n \n$f\n;\n \nfi\n;\n  \ndone\n;\n \ndone\n\n\nfor\n l in en de\n;\n \ndo\n \nfor\n f in data/multi30k/*.\n$l\n;\n \ndo\n perl tools/tokenizer.perl -a -no-escape -l \n$l\n -q  \n \n$f\n \n \n$f\n.atok\n;\n \ndone\n;\n \ndone\n\npython preprocess.py -train_src data/multi30k/train.en.atok -train_tgt data/multi30k/train.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k/val.de.atok -save_data data/multi30k.atok.low -lower\n\n\n\n\n\n2) Train the model.\n\n\npython train.py -data data/multi30k.atok.low -save_model multi30k_model -gpuid \n0\n\n\n\n\n\n\n3) Translate sentences.\n\n\npython translate.py -gpu \n0\n -model multi30k_model_*_e13.pt -src data/multi30k/test.en.atok -tgt data/multi30k/test.de.atok -replace_unk -verbose -output multi30k.test.pred.atok\n\n\n\n\n\n4) Evaluate.\n\n\nperl tools/multi-bleu.perl data/multi30k/test.de.atok \n multi30k.test.pred.atok", 
            "title": "Translation"
        }, 
        {
            "location": "/extended/#0-download-the-data", 
            "text": "mkdir -p data/multi30k\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz    tar -xf training.tar.gz -C data/multi30k   rm training.tar.gz\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz   tar -xf validation.tar.gz -C data/multi30k   rm validation.tar.gz\nwget http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz   tar -xf mmt_task1_test2016.tar.gz -C data/multi30k   rm mmt_task1_test2016.tar.gz", 
            "title": "0) Download the data."
        }, 
        {
            "location": "/extended/#1-preprocess-the-data", 
            "text": "# Delete the last line of val and training files.  for  l in en de ;   do   for  f in data/multi30k/*. $l ;   do   if   [[   $f  ! =  * test *  ]] ;   then  sed -i  $  d   $f ;   fi ;    done ;   done  for  l in en de ;   do   for  f in data/multi30k/*. $l ;   do  perl tools/tokenizer.perl -a -no-escape -l  $l  -q     $f     $f .atok ;   done ;   done \npython preprocess.py -train_src data/multi30k/train.en.atok -train_tgt data/multi30k/train.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k/val.de.atok -save_data data/multi30k.atok.low -lower", 
            "title": "1) Preprocess the data."
        }, 
        {
            "location": "/extended/#2-train-the-model", 
            "text": "python train.py -data data/multi30k.atok.low -save_model multi30k_model -gpuid  0", 
            "title": "2) Train the model."
        }, 
        {
            "location": "/extended/#3-translate-sentences", 
            "text": "python translate.py -gpu  0  -model multi30k_model_*_e13.pt -src data/multi30k/test.en.atok -tgt data/multi30k/test.de.atok -replace_unk -verbose -output multi30k.test.pred.atok", 
            "title": "3) Translate sentences."
        }, 
        {
            "location": "/extended/#4-evaluate", 
            "text": "perl tools/multi-bleu.perl data/multi30k/test.de.atok   multi30k.test.pred.atok", 
            "title": "4) Evaluate."
        }, 
        {
            "location": "/Summarization/", 
            "text": "Summarization Experiment Description\n\n\nThis document describes how to replicate summarization experiments on the CNNDM and gigaword datasets using OpenNMT-py. \nIn the following, we assume access to a tokenized form of the corpus split into train/valid/test set.\n\n\nAn example article-title pair from Gigaword should look like this: \n\n\nInput\n\n\naustralia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\n\n\nOutput\n\n\naustralian current account deficit narrows sharply\n\n\nPreprocessing the data\n\n\nSince we are using copy-attention [1] in the model, we need to preprocess the dataset such that source and target are aligned and use the same dictionary. This is achieved by using the options \ndynamic_dict\n and \nshare_vocab\n. \nWe additionally turn off truncation of the source to ensure that inputs longer than 50 words are not truncated. \nFor CNNDM we follow See et al. [2] and additionally truncate the source length at 400 tokens and the target at 100.  \n\n\ncommand used\n: \n\n\n(1) CNNDM\n\n\npython preprocess.py -train_src data/cnndm/train.txt.src -train_tgt data/cnn-no-sent-tag/train.txt.tgt -valid_src data/cnndm/val.txt.src -valid_tgt data/cnn-no-sent-tag/val.txt.tgt -save_data data/cnn-no-sent-tag/cnndm -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab\n\n\n(2) Gigaword\n\n\npython preprocess.py -train_src data/giga/train.article.txt -train_tgt data/giga/train.title.txt -valid_src data/giga/valid.article.txt -valid_tgt data/giga/valid.title.txt -save_data data/giga/giga -src_seq_length 10000 -dynamic_dict -share_vocab\n\n\nTraining\n\n\nThe training procedure described in this section for the most part follows parameter choices and implementation similar to that of See et al. [2].  As mentioned above, we use copy attention as a mechanism for the model to decide whether to either generate a new word or to copy from the source (\ncopy_attn\n).\nA notable difference to See's model is that we are using the attention mechanism introduced by Bahdanau et al. [3] (\nglobal_attention mlp\n) instead of that by Luong et al. [4] (\nglobal_attention dot\n). Both options typically perform very similar to each other with Luong attention often having a slight advantage.\nWe are using using a 128-dimensional word-embedding, and 512-dimensional 1 layer LSTM. On the encoder side, we use a bidirectional LSTM (\nbrnn\n), which means that the 512 dimensions are split into 256 dimensions per direction.\nWe also share the word embeddings between encoder and decoder (\nshare_embeddings\n). This option drastically reduces the number of parameters the model has to learn. However, we found only minimal impact on performance of a model without this option. \n\n\nFor the training procedure, we are using SGD with an initial learning rate of 1 for a total of 16 epochs. In most cases, the lowest validation perplexity is achieved around epoch 10-12. We also use OpenNMT's default learning rate decay, which halves the learning rate after every epoch once the validation perplexity increased after an epoch (or after epoch 8). \nAlternative training procedures such as adam with initial learning rate 0.001 converge faster than sgd, but achieve slightly worse. We additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value. \n\n\ncommands used\n: \n\n\n(1) CNNDM\n\n\npython train.py -save_model logs/notag_sgd3 -data data/cnn-no-sent-tag/CNNDM -copy_attn -global_attention mlp -word_vec_size 128 -rnn_size 256 -layers 1 -brnn -epochs 16 -seed 777 -batch_size 32 -max_grad_norm 2 -share_embeddings -gpuid 0 -start_checkpoint_at 9\n\n\n(2) Gigaword\n\n\npython train.py -save_model logs/giga_sgd3_512 -data data/giga/giga -copy_attn -global_attention mlp -word_vec_size 128 -rnn_size 512 -layers 1 -brnn -epochs 16 -seed 777 -batch_size 32 -max_grad_norm 2 -share_embeddings -gpuid 0 -start_checkpoint_at 9\n\n\nInference\n\n\nDuring inference, we use beam-search with a beam-size of 10. \nWe additionally use the \nreplace_unk\n option which replaces generated \nUNK\n tokens with the source token of highest attention. This acts as safety-net should the copy attention fail which should learn to copy such words.\n\n\ncommands used\n: \n\n\n(1) CNNDM\n\n\npython translate.py -gpu 2 -batch_size 1 -model logs/notag_try3_acc_49.29_ppl_14.62_e16.pt -src data/cnndm/test.txt.src -output sgd3_out.txt -beam_size 10 -replace_unk\n\n\n(2) Gigaword\n\n\npython translate.py -gpu 2 -batch_size 1 -model logs/giga_sgd3_512_acc_51.10_ppl_12.04_e16.pt -src data/giga/test.article.txt -output giga_sgd3.out.txt -beam_size 10 -replace_unk\n\n\nEvaluation\n\n\nCNNDM\n\n\nTo evaluate the ROUGE scores on CNNDM, we extended the pyrouge wrapper with additional evaluations such as the amount of repeated n-grams (typically found in models with copy attention), found \nhere\n. \n\n\nIt can be run with the following command:\n\n\npython baseline.py -s sgd3_out.txt -t ~/datasets/cnn-dailymail/sent-tagged/test.txt.tgt -m no_sent_tag -r\n\n\nNote that the \nno_sent_tag\n option strips tags around sentences - when a sentence previously was \ns\n w w w w . \n/s\n, it becomes \nw w w w .\n.\n\n\nGigaword\n\n\nFor evaluation of large test sets such as Gigaword, we use the a parallel python wrapper around ROUGE, found \nhere\n.\n\n\ncommand used\n: \n\nfiles2rouge giga_sgd3.out.txt test.title.txt --verbose\n\n\nRunning the commands above should yield the following scores:\n\n\nROUGE-1 (F): 0.352127\nROUGE-2 (F): 0.173109\nROUGE-3 (F): 0.098244\nROUGE-L (F): 0.327742\nROUGE-S4 (F): 0.155524\n\n\n\n\n\nReferences\n\n\n[1] Vinyals, O., Fortunato, M. and Jaitly, N., 2015. Pointer Network. NIPS\n\n\n[2] See, A., Liu, P.J. and Manning, C.D., 2017. Get To The Point: Summarization with Pointer-Generator Networks. ACL\n\n\n[3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. ICLR\n\n\n[4] Luong, M.T., Pham, H. and Manning, C.D., 2015. Effective approaches to attention-based neural machine translation. EMNLP", 
            "title": "Summarization"
        }, 
        {
            "location": "/Summarization/#summarization-experiment-description", 
            "text": "This document describes how to replicate summarization experiments on the CNNDM and gigaword datasets using OpenNMT-py. \nIn the following, we assume access to a tokenized form of the corpus split into train/valid/test set.  An example article-title pair from Gigaword should look like this:   Input  australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .  Output  australian current account deficit narrows sharply", 
            "title": "Summarization Experiment Description"
        }, 
        {
            "location": "/Summarization/#preprocessing-the-data", 
            "text": "Since we are using copy-attention [1] in the model, we need to preprocess the dataset such that source and target are aligned and use the same dictionary. This is achieved by using the options  dynamic_dict  and  share_vocab . \nWe additionally turn off truncation of the source to ensure that inputs longer than 50 words are not truncated. \nFor CNNDM we follow See et al. [2] and additionally truncate the source length at 400 tokens and the target at 100.    command used :   (1) CNNDM  python preprocess.py -train_src data/cnndm/train.txt.src -train_tgt data/cnn-no-sent-tag/train.txt.tgt -valid_src data/cnndm/val.txt.src -valid_tgt data/cnn-no-sent-tag/val.txt.tgt -save_data data/cnn-no-sent-tag/cnndm -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab  (2) Gigaword  python preprocess.py -train_src data/giga/train.article.txt -train_tgt data/giga/train.title.txt -valid_src data/giga/valid.article.txt -valid_tgt data/giga/valid.title.txt -save_data data/giga/giga -src_seq_length 10000 -dynamic_dict -share_vocab", 
            "title": "Preprocessing the data"
        }, 
        {
            "location": "/Summarization/#training", 
            "text": "The training procedure described in this section for the most part follows parameter choices and implementation similar to that of See et al. [2].  As mentioned above, we use copy attention as a mechanism for the model to decide whether to either generate a new word or to copy from the source ( copy_attn ).\nA notable difference to See's model is that we are using the attention mechanism introduced by Bahdanau et al. [3] ( global_attention mlp ) instead of that by Luong et al. [4] ( global_attention dot ). Both options typically perform very similar to each other with Luong attention often having a slight advantage.\nWe are using using a 128-dimensional word-embedding, and 512-dimensional 1 layer LSTM. On the encoder side, we use a bidirectional LSTM ( brnn ), which means that the 512 dimensions are split into 256 dimensions per direction.\nWe also share the word embeddings between encoder and decoder ( share_embeddings ). This option drastically reduces the number of parameters the model has to learn. However, we found only minimal impact on performance of a model without this option.   For the training procedure, we are using SGD with an initial learning rate of 1 for a total of 16 epochs. In most cases, the lowest validation perplexity is achieved around epoch 10-12. We also use OpenNMT's default learning rate decay, which halves the learning rate after every epoch once the validation perplexity increased after an epoch (or after epoch 8). \nAlternative training procedures such as adam with initial learning rate 0.001 converge faster than sgd, but achieve slightly worse. We additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value.   commands used :   (1) CNNDM  python train.py -save_model logs/notag_sgd3 -data data/cnn-no-sent-tag/CNNDM -copy_attn -global_attention mlp -word_vec_size 128 -rnn_size 256 -layers 1 -brnn -epochs 16 -seed 777 -batch_size 32 -max_grad_norm 2 -share_embeddings -gpuid 0 -start_checkpoint_at 9  (2) Gigaword  python train.py -save_model logs/giga_sgd3_512 -data data/giga/giga -copy_attn -global_attention mlp -word_vec_size 128 -rnn_size 512 -layers 1 -brnn -epochs 16 -seed 777 -batch_size 32 -max_grad_norm 2 -share_embeddings -gpuid 0 -start_checkpoint_at 9", 
            "title": "Training"
        }, 
        {
            "location": "/Summarization/#inference", 
            "text": "During inference, we use beam-search with a beam-size of 10. \nWe additionally use the  replace_unk  option which replaces generated  UNK  tokens with the source token of highest attention. This acts as safety-net should the copy attention fail which should learn to copy such words.  commands used :   (1) CNNDM  python translate.py -gpu 2 -batch_size 1 -model logs/notag_try3_acc_49.29_ppl_14.62_e16.pt -src data/cnndm/test.txt.src -output sgd3_out.txt -beam_size 10 -replace_unk  (2) Gigaword  python translate.py -gpu 2 -batch_size 1 -model logs/giga_sgd3_512_acc_51.10_ppl_12.04_e16.pt -src data/giga/test.article.txt -output giga_sgd3.out.txt -beam_size 10 -replace_unk", 
            "title": "Inference"
        }, 
        {
            "location": "/Summarization/#evaluation", 
            "text": "", 
            "title": "Evaluation"
        }, 
        {
            "location": "/Summarization/#cnndm", 
            "text": "To evaluate the ROUGE scores on CNNDM, we extended the pyrouge wrapper with additional evaluations such as the amount of repeated n-grams (typically found in models with copy attention), found  here .   It can be run with the following command:  python baseline.py -s sgd3_out.txt -t ~/datasets/cnn-dailymail/sent-tagged/test.txt.tgt -m no_sent_tag -r  Note that the  no_sent_tag  option strips tags around sentences - when a sentence previously was  s  w w w w .  /s , it becomes  w w w w . .", 
            "title": "CNNDM"
        }, 
        {
            "location": "/Summarization/#gigaword", 
            "text": "For evaluation of large test sets such as Gigaword, we use the a parallel python wrapper around ROUGE, found  here .  command used :  files2rouge giga_sgd3.out.txt test.title.txt --verbose  Running the commands above should yield the following scores:  ROUGE-1 (F): 0.352127\nROUGE-2 (F): 0.173109\nROUGE-3 (F): 0.098244\nROUGE-L (F): 0.327742\nROUGE-S4 (F): 0.155524", 
            "title": "Gigaword"
        }, 
        {
            "location": "/Summarization/#references", 
            "text": "[1] Vinyals, O., Fortunato, M. and Jaitly, N., 2015. Pointer Network. NIPS  [2] See, A., Liu, P.J. and Manning, C.D., 2017. Get To The Point: Summarization with Pointer-Generator Networks. ACL  [3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. ICLR  [4] Luong, M.T., Pham, H. and Manning, C.D., 2015. Effective approaches to attention-based neural machine translation. EMNLP", 
            "title": "References"
        }, 
        {
            "location": "/im2text/", 
            "text": "Image to Text\n\n\nA deep learning-based approach to learning the image-to-text conversion, built on top of the \nOpenNMT\n system. It is completely data-driven, hence can be used for a variety of image-to-text problems, such as image captioning, optical character recognition and LaTeX decompilation. \n\n\nTake LaTeX decompilation as an example, given a formula image:\n\n\n\n\n\nThe goal is to infer the LaTeX source that can be compiled to such an image:\n\n\n d s _ { 1 1 } ^ { 2 } = d x ^ { + } d x ^ { - } + l _ { p } ^ { 9 } \\frac { p _ { - } } { r ^ { 7 } } \\delta ( x ^ { - } ) d x ^ { - } d x ^ { - } + d x _ { 1 } ^ { 2 } + \\; \\cdots \\; + d x _ { 9 } ^ { 2 } \n\n\n\n\n\nThe paper \n[What You Get Is What You See: A Visual Markup Decompiler]\n provides more technical details of this model.\n\n\nDependencies\n\n\n\n\ntorchvision\n: \nconda install torchvision\n\n\nPillow\n: \npip install Pillow\n\n\n\n\nQuick Start\n\n\nTo get started, we provide a toy Math-to-LaTex example. We assume that the working directory is \nOpenNMT-py\n throughout this document.\n\n\nIm2Text consists of four commands:\n\n\n0) Download the data.\n\n\nwget -O data/im2text.tgz http://lstm.seas.harvard.edu/latex/im2text_small.tgz; tar zxf data/im2text.tgz -C data/\n\n\n\n\n\n1) Preprocess the data.\n\n\npython preprocess.py -data_type img -src_dir data/im2text/images/ -train_src data/im2text/src-train.txt -train_tgt data/im2text/tgt-train.txt -valid_src data/im2text/src-val.txt -valid_tgt data/im2text/tgt-val.txt -save_data data/im2text/demo\n\n\n\n\n\n2) Train the model.\n\n\npython train.py -model_type img -data data/im2text/demo -save_model demo-model -gpuid 0 -batch_size 20 -max_grad_norm 20 -learning_rate 0.1\n\n\n\n\n\n3) Translate the images.\n\n\npython translate.py -data_type img -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/im2text/images -src data/im2text/src-test.txt -output pred.txt -gpu 0 -verbose\n\n\n\n\n\nThe above dataset is sampled from the \nim2latex-100k-dataset\n. We provide a trained model \n[link]\n on this dataset.\n\n\nOptions\n\n\n\n\n\n\n-src_dir\n: The directory containing the images.\n\n\n\n\n\n\n-train_tgt\n: The file storing the tokenized labels, one label per line. It shall look like:\n\n\n\n\n\n\nlabel0_token0\n \nlabel0_token1\n ... \nlabel0_tokenN0\n\n\nlabel1_token0\n \nlabel1_token1\n ... \nlabel1_tokenN1\n\n\nlabel2_token0\n \nlabel2_token1\n ... \nlabel2_tokenN2\n\n...\n\n\n\n\n\n\n\n-train_src\n: The file storing the paths of the images (relative to \nsrc_dir\n).\n\n\n\n\nimage0_path\n\n\nimage1_path\n\n\nimage2_path\n\n...", 
            "title": "Image-to-Text"
        }, 
        {
            "location": "/im2text/#image-to-text", 
            "text": "A deep learning-based approach to learning the image-to-text conversion, built on top of the  OpenNMT  system. It is completely data-driven, hence can be used for a variety of image-to-text problems, such as image captioning, optical character recognition and LaTeX decompilation.   Take LaTeX decompilation as an example, given a formula image:   The goal is to infer the LaTeX source that can be compiled to such an image:   d s _ { 1 1 } ^ { 2 } = d x ^ { + } d x ^ { - } + l _ { p } ^ { 9 } \\frac { p _ { - } } { r ^ { 7 } } \\delta ( x ^ { - } ) d x ^ { - } d x ^ { - } + d x _ { 1 } ^ { 2 } + \\; \\cdots \\; + d x _ { 9 } ^ { 2 }   The paper  [What You Get Is What You See: A Visual Markup Decompiler]  provides more technical details of this model.", 
            "title": "Image to Text"
        }, 
        {
            "location": "/im2text/#dependencies", 
            "text": "torchvision :  conda install torchvision  Pillow :  pip install Pillow", 
            "title": "Dependencies"
        }, 
        {
            "location": "/im2text/#quick-start", 
            "text": "To get started, we provide a toy Math-to-LaTex example. We assume that the working directory is  OpenNMT-py  throughout this document.  Im2Text consists of four commands:  0) Download the data.  wget -O data/im2text.tgz http://lstm.seas.harvard.edu/latex/im2text_small.tgz; tar zxf data/im2text.tgz -C data/  1) Preprocess the data.  python preprocess.py -data_type img -src_dir data/im2text/images/ -train_src data/im2text/src-train.txt -train_tgt data/im2text/tgt-train.txt -valid_src data/im2text/src-val.txt -valid_tgt data/im2text/tgt-val.txt -save_data data/im2text/demo  2) Train the model.  python train.py -model_type img -data data/im2text/demo -save_model demo-model -gpuid 0 -batch_size 20 -max_grad_norm 20 -learning_rate 0.1  3) Translate the images.  python translate.py -data_type img -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/im2text/images -src data/im2text/src-test.txt -output pred.txt -gpu 0 -verbose  The above dataset is sampled from the  im2latex-100k-dataset . We provide a trained model  [link]  on this dataset.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/im2text/#options", 
            "text": "-src_dir : The directory containing the images.    -train_tgt : The file storing the tokenized labels, one label per line. It shall look like:    label0_token0   label0_token1  ...  label0_tokenN0  label1_token0   label1_token1  ...  label1_tokenN1  label2_token0   label2_token1  ...  label2_tokenN2 \n...   -train_src : The file storing the paths of the images (relative to  src_dir ).   image0_path  image1_path  image2_path \n...", 
            "title": "Options"
        }, 
        {
            "location": "/speech2text/", 
            "text": "Speech to Text\n\n\nA deep learning-based approach to learning the speech-to-text conversion, built on top of the \nOpenNMT\n system.\n\n\nGiven raw audio, we first apply short-time Fourier transform (STFT), then apply Convolutional Neural Networks to get the source features. Based on this source representation, we use an LSTM decoder with attention to produce the text character by character.\n\n\nDependencies\n\n\n\n\ntorchaudio\n: \nsudo apt-get install -y sox libsox-dev libsox-fmt-all; pip install git+https://github.com/pytorch/audio\n\n\nlibrosa\n: \npip install librosa\n\n\n\n\nQuick Start\n\n\nTo get started, we provide a toy speech-to-text example. We assume that the working directory is \nOpenNMT-py\n throughout this document.\n\n\n0) Download the data.\n\n\nwget -O data/speech.tgz http://lstm.seas.harvard.edu/latex/speech.tgz; tar zxf data/speech.tgz -C data/\n\n\n\n\n\n1) Preprocess the data.\n\n\npython preprocess.py -data_type audio -src_dir data/speech/an4_dataset -train_src data/speech/src-train.txt -train_tgt data/speech/tgt-train.txt -valid_src data/speech/src-val.txt -valid_tgt data/speech/tgt-val.txt -save_data data/speech/demo\n\n\n\n\n\n2) Train the model.\n\n\npython train.py -model_type audio -data data/speech/demo -save_model demo-model -gpuid 0 -batch_size 16 -max_grad_norm 20 -learning_rate 0.1 -learning_rate_decay 0.98 -epochs 60\n\n\n\n\n\n3) Translate the speechs.\n\n\npython translate.py -data_type audio -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/speech/an4_dataset -src data/speech/src-val.txt -output pred.txt -gpu 0 -verbose\n\n\n\n\n\nOptions\n\n\n\n\n\n\n-src_dir\n: The directory containing the audio files.\n\n\n\n\n\n\n-train_tgt\n: The file storing the tokenized labels, one label per line. It shall look like:\n\n\n\n\n\n\nlabel0_token0\n \nlabel0_token1\n ... \nlabel0_tokenN0\n\n\nlabel1_token0\n \nlabel1_token1\n ... \nlabel1_tokenN1\n\n\nlabel2_token0\n \nlabel2_token1\n ... \nlabel2_tokenN2\n\n...\n\n\n\n\n\n\n\n-train_src\n: The file storing the paths of the audio files (relative to \nsrc_dir\n).\n\n\n\n\nspeech0_path\n\n\nspeech1_path\n\n\nspeech2_path\n\n...\n\n\n\n\n\n\n\nsample_rate\n: Sample rate. Default: 16000.\n\n\nwindow_size\n: Window size for spectrogram in seconds. Default: 0.02.\n\n\nwindow_stride\n: Window stride for spectrogram in seconds. Default: 0.01.\n\n\nwindow\n: Window type for spectrogram generation. Default: hamming.\n\n\n\n\nAcknowledgement\n\n\nOur preprocessing and CNN encoder is adapted from \ndeepspeech.pytorch\n.", 
            "title": "Speech-to-Text"
        }, 
        {
            "location": "/speech2text/#speech-to-text", 
            "text": "A deep learning-based approach to learning the speech-to-text conversion, built on top of the  OpenNMT  system.  Given raw audio, we first apply short-time Fourier transform (STFT), then apply Convolutional Neural Networks to get the source features. Based on this source representation, we use an LSTM decoder with attention to produce the text character by character.", 
            "title": "Speech to Text"
        }, 
        {
            "location": "/speech2text/#dependencies", 
            "text": "torchaudio :  sudo apt-get install -y sox libsox-dev libsox-fmt-all; pip install git+https://github.com/pytorch/audio  librosa :  pip install librosa", 
            "title": "Dependencies"
        }, 
        {
            "location": "/speech2text/#quick-start", 
            "text": "To get started, we provide a toy speech-to-text example. We assume that the working directory is  OpenNMT-py  throughout this document.  0) Download the data.  wget -O data/speech.tgz http://lstm.seas.harvard.edu/latex/speech.tgz; tar zxf data/speech.tgz -C data/  1) Preprocess the data.  python preprocess.py -data_type audio -src_dir data/speech/an4_dataset -train_src data/speech/src-train.txt -train_tgt data/speech/tgt-train.txt -valid_src data/speech/src-val.txt -valid_tgt data/speech/tgt-val.txt -save_data data/speech/demo  2) Train the model.  python train.py -model_type audio -data data/speech/demo -save_model demo-model -gpuid 0 -batch_size 16 -max_grad_norm 20 -learning_rate 0.1 -learning_rate_decay 0.98 -epochs 60  3) Translate the speechs.  python translate.py -data_type audio -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/speech/an4_dataset -src data/speech/src-val.txt -output pred.txt -gpu 0 -verbose", 
            "title": "Quick Start"
        }, 
        {
            "location": "/speech2text/#options", 
            "text": "-src_dir : The directory containing the audio files.    -train_tgt : The file storing the tokenized labels, one label per line. It shall look like:    label0_token0   label0_token1  ...  label0_tokenN0  label1_token0   label1_token1  ...  label1_tokenN1  label2_token0   label2_token1  ...  label2_tokenN2 \n...   -train_src : The file storing the paths of the audio files (relative to  src_dir ).   speech0_path  speech1_path  speech2_path \n...   sample_rate : Sample rate. Default: 16000.  window_size : Window size for spectrogram in seconds. Default: 0.02.  window_stride : Window stride for spectrogram in seconds. Default: 0.01.  window : Window type for spectrogram generation. Default: hamming.", 
            "title": "Options"
        }, 
        {
            "location": "/speech2text/#acknowledgement", 
            "text": "Our preprocessing and CNN encoder is adapted from  deepspeech.pytorch .", 
            "title": "Acknowledgement"
        }, 
        {
            "location": "/FAQ/", 
            "text": "How do I use Pretrained embeddings (e.g. GloVe)?\n\n\nUsing vocabularies from OpenNMT-py preprocessing outputs, \nembeddings_to_torch.py\n to generate encoder and decoder embeddings initialized with GloVe\u2019s values.\n\n\nthe script is a slightly modified version of ylhsieh\u2019s one2.\n\n\nUsage:\n\n\nembeddings_to_torch.py [-h] -emb_file EMB_FILE -output_file OUTPUT_FILE -dict_file DICT_FILE [-verbose]\n\nemb_file: GloVe like embedding file i.e. CSV [word] [dim1] ... [dim_d]\n\noutput_file: a filename to save the output as PyTorch serialized tensors2\n\ndict_file: dict output from OpenNMT-py preprocessing\n\n\n\n\n\nExample\n\n\n1) get GloVe files:\n\n\nmkdir \nglove_dir\n\nwget http://nlp.stanford.edu/data/glove.6B.zip\nunzip glove.6B.zip -d \nglove_dir\n\n\n\n\n\n\n2) prepare data:\n\n\npython preprocess.py \\\n-train_src data/train.src.txt \\\n-train_tgt data/train.tgt.txt \\\n-valid_src data/valid.src.txt \\\n-valid_tgt data/valid.tgt.txt \\\n-save_data data/data\n\n\n\n\n\n3) prepare embeddings:\n\n\n./tools/embeddings_to_torch.py -emb_file \nglove_dir/glove.6B.100d.txt\n \\\n-dict_file \ndata/data.vocab.pt\n \\\n-output_file \ndata/embeddings\n\n\n\n\n\n\n4) train using pre-trained embeddings:\n\n\npython train.py -save_model data/model \\\n-batch_size 64 \\\n-layers 2 \\\n-rnn_size 200 \\\n-word_vec_size 100 \\\n-pre_word_vecs_enc \ndata/embeddings.enc.pt\n \\\n-pre_word_vecs_dec \ndata/embeddings.dec.pt\n \\\n        -data data/data\n\n\n\n\n\nHow do I use the Transformer model?\n\n\nThe transformer model is very sensitive to hyperparameters. To run it\neffectively you need to set a bunch of different options that mimic the Google\nsetup.\n\n\npython train.py -data mydata/data -save_model mydata/model -gpuid 0 -epochs 50 \n-layers 4 -rnn_size 1024 -word_vec_size 1024 -epochs 50 -max_grad_norm 0\n-optim adam -encoder_type transformer -decoder_type transformer\n-position_encoding -dropout 0.2 -param_init 0 -warmup_steps 2000\n-learning_rate 0.05 -decay_method noam\n\n\n\n\n\nDo you support multi-gpu?\n\n\nCurrently our system does not support multi-gpu. We do intend to do so in the future.", 
            "title": "FAQ"
        }, 
        {
            "location": "/FAQ/#how-do-i-use-pretrained-embeddings-eg-glove", 
            "text": "Using vocabularies from OpenNMT-py preprocessing outputs,  embeddings_to_torch.py  to generate encoder and decoder embeddings initialized with GloVe\u2019s values.  the script is a slightly modified version of ylhsieh\u2019s one2.  Usage:  embeddings_to_torch.py [-h] -emb_file EMB_FILE -output_file OUTPUT_FILE -dict_file DICT_FILE [-verbose]\n\nemb_file: GloVe like embedding file i.e. CSV [word] [dim1] ... [dim_d]\n\noutput_file: a filename to save the output as PyTorch serialized tensors2\n\ndict_file: dict output from OpenNMT-py preprocessing  Example  1) get GloVe files:  mkdir  glove_dir \nwget http://nlp.stanford.edu/data/glove.6B.zip\nunzip glove.6B.zip -d  glove_dir   2) prepare data:  python preprocess.py \\\n-train_src data/train.src.txt \\\n-train_tgt data/train.tgt.txt \\\n-valid_src data/valid.src.txt \\\n-valid_tgt data/valid.tgt.txt \\\n-save_data data/data  3) prepare embeddings:  ./tools/embeddings_to_torch.py -emb_file  glove_dir/glove.6B.100d.txt  \\\n-dict_file  data/data.vocab.pt  \\\n-output_file  data/embeddings   4) train using pre-trained embeddings:  python train.py -save_model data/model \\\n-batch_size 64 \\\n-layers 2 \\\n-rnn_size 200 \\\n-word_vec_size 100 \\\n-pre_word_vecs_enc  data/embeddings.enc.pt  \\\n-pre_word_vecs_dec  data/embeddings.dec.pt  \\\n        -data data/data", 
            "title": "How do I use Pretrained embeddings (e.g. GloVe)?"
        }, 
        {
            "location": "/FAQ/#how-do-i-use-the-transformer-model", 
            "text": "The transformer model is very sensitive to hyperparameters. To run it\neffectively you need to set a bunch of different options that mimic the Google\nsetup.  python train.py -data mydata/data -save_model mydata/model -gpuid 0 -epochs 50 \n-layers 4 -rnn_size 1024 -word_vec_size 1024 -epochs 50 -max_grad_norm 0\n-optim adam -encoder_type transformer -decoder_type transformer\n-position_encoding -dropout 0.2 -param_init 0 -warmup_steps 2000\n-learning_rate 0.05 -decay_method noam", 
            "title": "How do I use the Transformer model?"
        }, 
        {
            "location": "/FAQ/#do-you-support-multi-gpu", 
            "text": "Currently our system does not support multi-gpu. We do intend to do so in the future.", 
            "title": "Do you support multi-gpu?"
        }, 
        {
            "location": "/options/preprocess/", 
            "text": "preprocess.py\n:\npreprocess.py\n\n\nData\n:\n\n\n\n\n\n\n-data_type [text]\n \nType of the source input. Options are [text|img].\n\n\n\n\n\n\n-train_src \n \nPath to the training source data\n\n\n\n\n\n\n-train_tgt \n \nPath to the training target data\n\n\n\n\n\n\n-valid_src \n \nPath to the validation source data\n\n\n\n\n\n\n-valid_tgt \n \nPath to the validation target data\n\n\n\n\n\n\n-src_dir \n \nSource directory for image or audio files.\n\n\n\n\n\n\n-save_data \n \nOutput file for the prepared data\n\n\n\n\n\n\nVocab\n:\n\n\n\n\n\n\n-src_vocab \n \nPath to an existing source vocabulary\n\n\n\n\n\n\n-tgt_vocab \n \nPath to an existing target vocabulary\n\n\n\n\n\n\n-features_vocabs_prefix \n \nPath prefix to existing features vocabularies\n\n\n\n\n\n\n-src_vocab_size [50000]\n \nSize of the source vocabulary\n\n\n\n\n\n\n-tgt_vocab_size [50000]\n \nSize of the target vocabulary\n\n\n\n\n\n\n-src_words_min_frequency \n \n\n\n\n\n\n\n-tgt_words_min_frequency \n \n\n\n\n\n\n\n-dynamic_dict \n \nCreate dynamic dictionaries\n\n\n\n\n\n\n-share_vocab \n \nShare source and target vocabulary\n\n\n\n\n\n\nPruning\n:\n\n\n\n\n\n\n-src_seq_length [50]\n \nMaximum source sequence length\n\n\n\n\n\n\n-src_seq_length_trunc \n \nTruncate source sequence length.\n\n\n\n\n\n\n-tgt_seq_length [50]\n \nMaximum target sequence length to keep.\n\n\n\n\n\n\n-tgt_seq_length_trunc \n \nTruncate target sequence length.\n\n\n\n\n\n\n-lower \n \nlowercase data\n\n\n\n\n\n\nRandom\n:\n\n\n\n\n\n\n-shuffle [1]\n \nShuffle data\n\n\n\n\n\n\n-seed [3435]\n \nRandom seed\n\n\n\n\n\n\nLogging\n:\n\n\n\n\n-report_every [100000]\n \nReport status every this many sentences\n\n\n\n\nSpeech\n:\n\n\n\n\n\n\n-sample_rate [16000]\n \nSample rate.\n\n\n\n\n\n\n-window_size [0.02]\n \nWindow size for spectrogram in seconds.\n\n\n\n\n\n\n-window_stride [0.01]\n \nWindow stride for spectrogram in seconds.\n\n\n\n\n\n\n-window [hamming]\n \nWindow type for spectrogram generation.", 
            "title": "preprocess.py"
        }, 
        {
            "location": "/options/preprocess/#data", 
            "text": "-data_type [text]  \nType of the source input. Options are [text|img].    -train_src   \nPath to the training source data    -train_tgt   \nPath to the training target data    -valid_src   \nPath to the validation source data    -valid_tgt   \nPath to the validation target data    -src_dir   \nSource directory for image or audio files.    -save_data   \nOutput file for the prepared data", 
            "title": "Data:"
        }, 
        {
            "location": "/options/preprocess/#vocab", 
            "text": "-src_vocab   \nPath to an existing source vocabulary    -tgt_vocab   \nPath to an existing target vocabulary    -features_vocabs_prefix   \nPath prefix to existing features vocabularies    -src_vocab_size [50000]  \nSize of the source vocabulary    -tgt_vocab_size [50000]  \nSize of the target vocabulary    -src_words_min_frequency       -tgt_words_min_frequency       -dynamic_dict   \nCreate dynamic dictionaries    -share_vocab   \nShare source and target vocabulary", 
            "title": "Vocab:"
        }, 
        {
            "location": "/options/preprocess/#pruning", 
            "text": "-src_seq_length [50]  \nMaximum source sequence length    -src_seq_length_trunc   \nTruncate source sequence length.    -tgt_seq_length [50]  \nMaximum target sequence length to keep.    -tgt_seq_length_trunc   \nTruncate target sequence length.    -lower   \nlowercase data", 
            "title": "Pruning:"
        }, 
        {
            "location": "/options/preprocess/#random", 
            "text": "-shuffle [1]  \nShuffle data    -seed [3435]  \nRandom seed", 
            "title": "Random:"
        }, 
        {
            "location": "/options/preprocess/#logging", 
            "text": "-report_every [100000]  \nReport status every this many sentences", 
            "title": "Logging:"
        }, 
        {
            "location": "/options/preprocess/#speech", 
            "text": "-sample_rate [16000]  \nSample rate.    -window_size [0.02]  \nWindow size for spectrogram in seconds.    -window_stride [0.01]  \nWindow stride for spectrogram in seconds.    -window [hamming]  \nWindow type for spectrogram generation.", 
            "title": "Speech:"
        }, 
        {
            "location": "/options/train/", 
            "text": "train.py\n:\ntrain.py\n\n\nModel-Embeddings\n:\n\n\n\n\n\n\n-word_vec_size [-1]\n \nWord embedding for both.\n\n\n\n\n\n\n-src_word_vec_size [500]\n \nSrc word embedding sizes\n\n\n\n\n\n\n-tgt_word_vec_size [500]\n \nTgt word embedding sizes\n\n\n\n\n\n\n-feat_merge [concat]\n \nMerge action for the features embeddings\n\n\n\n\n\n\n-feat_vec_size [-1]\n \nIf specified, feature embedding sizes will be set to this. Otherwise,\nfeat_vec_exponent will be used.\n\n\n\n\n\n\n-feat_vec_exponent [0.7]\n \nIf -feat_merge_size is not set, feature embedding sizes will be set to\nN^feat_vec_exponent where N is the number of values the feature takes.\n\n\n\n\n\n\n-position_encoding \n \nUse a sin to mark relative words positions.\n\n\n\n\n\n\n-share_decoder_embeddings \n \nShare the word and out embeddings for decoder.\n\n\n\n\n\n\n-share_embeddings \n \nShare the word embeddings between encoder and decoder.\n\n\n\n\n\n\nModel- Encoder-Decoder\n:\n\n\n\n\n\n\n-model_type [text]\n \nType of encoder to use. Options are [text|img|audio].\n\n\n\n\n\n\n-encoder_type [rnn]\n \nType of encoder layer to use.\n\n\n\n\n\n\n-decoder_type [rnn]\n \nType of decoder layer to use.\n\n\n\n\n\n\n-layers [-1]\n \nNumber of layers in enc/dec.\n\n\n\n\n\n\n-enc_layers [2]\n \nNumber of layers in the encoder\n\n\n\n\n\n\n-dec_layers [2]\n \nNumber of layers in the decoder\n\n\n\n\n\n\n-cnn_kernel_width [3]\n \nSize of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in conv\nlayer\n\n\n\n\n\n\n-rnn_size [500]\n \nSize of LSTM hidden states\n\n\n\n\n\n\n-input_feed [1]\n \nFeed the context vector at each time step as additional input (via concatenation\nwith the word embeddings) to the decoder.\n\n\n\n\n\n\n-rnn_type [LSTM]\n \nThe gate type to use in the RNNs\n\n\n\n\n\n\n-brnn \n \nDeprecated, use \nencoder_type\n.\n\n\n\n\n\n\n-brnn_merge [concat]\n \nMerge action for the bidir hidden states\n\n\n\n\n\n\n-context_gate \n \nType of context gate to use. Do not select for no context gate.\n\n\n\n\n\n\nModel- Attention\n:\n\n\n\n\n\n\n-global_attention [general]\n \nThe attention type to use: dotprod or general (Luong) or MLP (Bahdanau)\n\n\n\n\n\n\n-copy_attn \n \nTrain copy attention layer.\n\n\n\n\n\n\n-copy_attn_force \n \nWhen available, train to copy.\n\n\n\n\n\n\n-coverage_attn \n \nTrain a coverage attention layer.\n\n\n\n\n\n\n-lambda_coverage [1]\n \nLambda value for coverage.\n\n\n\n\n\n\nGeneral\n:\n\n\n\n\n\n\n-data \n \nPath prefix to the \".train.pt\" and \".valid.pt\" file path from preprocess.py\n\n\n\n\n\n\n-save_model [model]\n \nModel filename (the model will be saved as \n_epochN_PPL.pt where PPL\nis the validation perplexity\n\n\n\n\n\n\n-gpuid \n \nUse CUDA on the listed devices.\n\n\n\n\n\n\n-seed [-1]\n \nRandom seed used for the experiments reproducibility.\n\n\n\n\n\n\nInitialization\n:\n\n\n\n\n\n\n-start_epoch [1]\n \nThe epoch from which to start\n\n\n\n\n\n\n-param_init [0.1]\n \nParameters are initialized over uniform distribution with support (-param_init,\nparam_init). Use 0 to not use initialization\n\n\n\n\n\n\n-train_from \n \nIf training from a checkpoint then this is the path to the pretrained model's\nstate_dict.\n\n\n\n\n\n\n-pre_word_vecs_enc \n \nIf a valid path is specified, then this will load pretrained word embeddings on\nthe encoder side. See README for specific formatting instructions.\n\n\n\n\n\n\n-pre_word_vecs_dec \n \nIf a valid path is specified, then this will load pretrained word embeddings on\nthe decoder side. See README for specific formatting instructions.\n\n\n\n\n\n\n-fix_word_vecs_enc \n \nFix word embeddings on the encoder side.\n\n\n\n\n\n\n-fix_word_vecs_dec \n \nFix word embeddings on the encoder side.\n\n\n\n\n\n\nOptimization- Type\n:\n\n\n\n\n\n\n-batch_size [64]\n \nMaximum batch size\n\n\n\n\n\n\n-max_generator_batches [32]\n \nMaximum batches of words in a sequence to run the generator on in parallel.\nHigher is faster, but uses more memory.\n\n\n\n\n\n\n-epochs [13]\n \nNumber of training epochs\n\n\n\n\n\n\n-optim [sgd]\n \nOptimization method.\n\n\n\n\n\n\n-adagrad_accumulator_init \n \nInitializes the accumulator values in adagrad. Mirrors the\ninitial_accumulator_value option in the tensorflow adagrad (use 0.1 for their\ndefault).\n\n\n\n\n\n\n-max_grad_norm [5]\n \nIf the norm of the gradient vector exceeds this, renormalize it to have the norm\nequal to max_grad_norm\n\n\n\n\n\n\n-dropout [0.3]\n \nDropout probability; applied in LSTM stacks.\n\n\n\n\n\n\n-truncated_decoder \n \nTruncated bptt.\n\n\n\n\n\n\n-adam_beta1 [0.9]\n \nThe beta1 parameter used by Adam. Almost without exception a value of 0.9 is\nused in the literature, seemingly giving good results, so we would discourage\nchanging this value from the default without due consideration.\n\n\n\n\n\n\n-adam_beta2 [0.999]\n \nThe beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as\nthis is the value suggested by the original paper describing Adam, and is also\nthe value adopted in other frameworks such as Tensorflow and Kerras, i.e. see:\nhttps://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\nhttps://keras.io/optimizers/ . Whereas recently the paper \"Attention is All You\nNeed\" suggested a value of 0.98 for beta2, this parameter may not work well for\nnormal models / default baselines.\n\n\n\n\n\n\nOptimization- Rate\n:\n\n\n\n\n\n\n-learning_rate [1.0]\n \nStarting learning rate. Recommended settings: sgd = 1, adagrad = 0.1, adadelta =\n1, adam = 0.001\n\n\n\n\n\n\n-learning_rate_decay [0.5]\n \nIf update_learning_rate, decay learning rate by this much if (i) perplexity does\nnot decrease on the validation set or (ii) epoch has gone past start_decay_at\n\n\n\n\n\n\n-start_decay_at [8]\n \nStart decaying every epoch after and including this epoch\n\n\n\n\n\n\n-start_checkpoint_at \n \nStart checkpointing every epoch after and including this epoch\n\n\n\n\n\n\n-decay_method \n \nUse a custom decay rate.\n\n\n\n\n\n\n-warmup_steps [4000]\n \nNumber of warmup steps for custom decay.\n\n\n\n\n\n\nLogging\n:\n\n\n\n\n\n\n-report_every [50]\n \nPrint stats at this interval.\n\n\n\n\n\n\n-exp_host \n \nSend logs to this crayon server.\n\n\n\n\n\n\n-exp \n \nName of the experiment for logging.\n\n\n\n\n\n\nSpeech\n:\n\n\n\n\n\n\n-sample_rate [16000]\n \nSample rate.\n\n\n\n\n\n\n-window_size [0.02]\n \nWindow size for spectrogram in seconds.", 
            "title": "train.py"
        }, 
        {
            "location": "/options/train/#model-embeddings", 
            "text": "-word_vec_size [-1]  \nWord embedding for both.    -src_word_vec_size [500]  \nSrc word embedding sizes    -tgt_word_vec_size [500]  \nTgt word embedding sizes    -feat_merge [concat]  \nMerge action for the features embeddings    -feat_vec_size [-1]  \nIf specified, feature embedding sizes will be set to this. Otherwise,\nfeat_vec_exponent will be used.    -feat_vec_exponent [0.7]  \nIf -feat_merge_size is not set, feature embedding sizes will be set to\nN^feat_vec_exponent where N is the number of values the feature takes.    -position_encoding   \nUse a sin to mark relative words positions.    -share_decoder_embeddings   \nShare the word and out embeddings for decoder.    -share_embeddings   \nShare the word embeddings between encoder and decoder.", 
            "title": "Model-Embeddings:"
        }, 
        {
            "location": "/options/train/#model-encoder-decoder", 
            "text": "-model_type [text]  \nType of encoder to use. Options are [text|img|audio].    -encoder_type [rnn]  \nType of encoder layer to use.    -decoder_type [rnn]  \nType of decoder layer to use.    -layers [-1]  \nNumber of layers in enc/dec.    -enc_layers [2]  \nNumber of layers in the encoder    -dec_layers [2]  \nNumber of layers in the decoder    -cnn_kernel_width [3]  \nSize of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in conv\nlayer    -rnn_size [500]  \nSize of LSTM hidden states    -input_feed [1]  \nFeed the context vector at each time step as additional input (via concatenation\nwith the word embeddings) to the decoder.    -rnn_type [LSTM]  \nThe gate type to use in the RNNs    -brnn   \nDeprecated, use  encoder_type .    -brnn_merge [concat]  \nMerge action for the bidir hidden states    -context_gate   \nType of context gate to use. Do not select for no context gate.", 
            "title": "Model- Encoder-Decoder:"
        }, 
        {
            "location": "/options/train/#model-attention", 
            "text": "-global_attention [general]  \nThe attention type to use: dotprod or general (Luong) or MLP (Bahdanau)    -copy_attn   \nTrain copy attention layer.    -copy_attn_force   \nWhen available, train to copy.    -coverage_attn   \nTrain a coverage attention layer.    -lambda_coverage [1]  \nLambda value for coverage.", 
            "title": "Model- Attention:"
        }, 
        {
            "location": "/options/train/#general", 
            "text": "-data   \nPath prefix to the \".train.pt\" and \".valid.pt\" file path from preprocess.py    -save_model [model]  \nModel filename (the model will be saved as  _epochN_PPL.pt where PPL\nis the validation perplexity    -gpuid   \nUse CUDA on the listed devices.    -seed [-1]  \nRandom seed used for the experiments reproducibility.", 
            "title": "General:"
        }, 
        {
            "location": "/options/train/#initialization", 
            "text": "-start_epoch [1]  \nThe epoch from which to start    -param_init [0.1]  \nParameters are initialized over uniform distribution with support (-param_init,\nparam_init). Use 0 to not use initialization    -train_from   \nIf training from a checkpoint then this is the path to the pretrained model's\nstate_dict.    -pre_word_vecs_enc   \nIf a valid path is specified, then this will load pretrained word embeddings on\nthe encoder side. See README for specific formatting instructions.    -pre_word_vecs_dec   \nIf a valid path is specified, then this will load pretrained word embeddings on\nthe decoder side. See README for specific formatting instructions.    -fix_word_vecs_enc   \nFix word embeddings on the encoder side.    -fix_word_vecs_dec   \nFix word embeddings on the encoder side.", 
            "title": "Initialization:"
        }, 
        {
            "location": "/options/train/#optimization-type", 
            "text": "-batch_size [64]  \nMaximum batch size    -max_generator_batches [32]  \nMaximum batches of words in a sequence to run the generator on in parallel.\nHigher is faster, but uses more memory.    -epochs [13]  \nNumber of training epochs    -optim [sgd]  \nOptimization method.    -adagrad_accumulator_init   \nInitializes the accumulator values in adagrad. Mirrors the\ninitial_accumulator_value option in the tensorflow adagrad (use 0.1 for their\ndefault).    -max_grad_norm [5]  \nIf the norm of the gradient vector exceeds this, renormalize it to have the norm\nequal to max_grad_norm    -dropout [0.3]  \nDropout probability; applied in LSTM stacks.    -truncated_decoder   \nTruncated bptt.    -adam_beta1 [0.9]  \nThe beta1 parameter used by Adam. Almost without exception a value of 0.9 is\nused in the literature, seemingly giving good results, so we would discourage\nchanging this value from the default without due consideration.    -adam_beta2 [0.999]  \nThe beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as\nthis is the value suggested by the original paper describing Adam, and is also\nthe value adopted in other frameworks such as Tensorflow and Kerras, i.e. see:\nhttps://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\nhttps://keras.io/optimizers/ . Whereas recently the paper \"Attention is All You\nNeed\" suggested a value of 0.98 for beta2, this parameter may not work well for\nnormal models / default baselines.", 
            "title": "Optimization- Type:"
        }, 
        {
            "location": "/options/train/#optimization-rate", 
            "text": "-learning_rate [1.0]  \nStarting learning rate. Recommended settings: sgd = 1, adagrad = 0.1, adadelta =\n1, adam = 0.001    -learning_rate_decay [0.5]  \nIf update_learning_rate, decay learning rate by this much if (i) perplexity does\nnot decrease on the validation set or (ii) epoch has gone past start_decay_at    -start_decay_at [8]  \nStart decaying every epoch after and including this epoch    -start_checkpoint_at   \nStart checkpointing every epoch after and including this epoch    -decay_method   \nUse a custom decay rate.    -warmup_steps [4000]  \nNumber of warmup steps for custom decay.", 
            "title": "Optimization- Rate:"
        }, 
        {
            "location": "/options/train/#logging", 
            "text": "-report_every [50]  \nPrint stats at this interval.    -exp_host   \nSend logs to this crayon server.    -exp   \nName of the experiment for logging.", 
            "title": "Logging:"
        }, 
        {
            "location": "/options/train/#speech", 
            "text": "-sample_rate [16000]  \nSample rate.    -window_size [0.02]  \nWindow size for spectrogram in seconds.", 
            "title": "Speech:"
        }, 
        {
            "location": "/options/translate/", 
            "text": "translate.py\n:\ntranslate.py\n\n\nModel\n:\n\n\n\n\n-model \n \nPath to model .pt file\n\n\n\n\nData\n:\n\n\n\n\n\n\n-data_type [text]\n \nType of the source input. Options: [text|img].\n\n\n\n\n\n\n-src \n \nSource sequence to decode (one line per sequence)\n\n\n\n\n\n\n-src_dir \n \nSource directory for image or audio files\n\n\n\n\n\n\n-tgt \n \nTrue target sequence (optional)\n\n\n\n\n\n\n-output [pred.txt]\n \nPath to output the predictions (each line will be the decoded sequence\n\n\n\n\n\n\n-dynamic_dict \n \nCreate dynamic dictionaries\n\n\n\n\n\n\n-share_vocab \n \nShare source and target vocabulary\n\n\n\n\n\n\nBeam\n:\n\n\n\n\n\n\n-beam_size [5]\n \nBeam size\n\n\n\n\n\n\n-alpha \n \nGoogle NMT length penalty parameter (higher = longer generation)\n\n\n\n\n\n\n-beta \n \nCoverage penalty parameter\n\n\n\n\n\n\n-max_sent_length [100]\n \nMaximum sentence length.\n\n\n\n\n\n\n-replace_unk \n \nReplace the generated UNK tokens with the source token that had highest\nattention weight. If phrase_table is provided, it will lookup the identified\nsource token and give the corresponding target token. If it is not provided(or\nthe identified source token does not exist in the table) then it will copy the\nsource token\n\n\n\n\n\n\nLogging\n:\n\n\n\n\n\n\n-verbose \n \nPrint scores and predictions for each sentence\n\n\n\n\n\n\n-attn_debug \n \nPrint best attn for each word\n\n\n\n\n\n\n-dump_beam \n \nFile to dump beam information to.\n\n\n\n\n\n\n-n_best [1]\n \nIf verbose is set, will output the n_best decoded sentences\n\n\n\n\n\n\nEfficiency\n:\n\n\n\n\n\n\n-batch_size [30]\n \nBatch size\n\n\n\n\n\n\n-gpu [-1]\n \nDevice to run on\n\n\n\n\n\n\nSpeech\n:\n\n\n\n\n\n\n-sample_rate [16000]\n \nSample rate.\n\n\n\n\n\n\n-window_size [0.02]\n \nWindow size for spectrogram in seconds\n\n\n\n\n\n\n-window_stride [0.01]\n \nWindow stride for spectrogram in seconds\n\n\n\n\n\n\n-window [hamming]\n \nWindow type for spectrogram generation", 
            "title": "translate.py"
        }, 
        {
            "location": "/options/translate/#model", 
            "text": "-model   \nPath to model .pt file", 
            "title": "Model:"
        }, 
        {
            "location": "/options/translate/#data", 
            "text": "-data_type [text]  \nType of the source input. Options: [text|img].    -src   \nSource sequence to decode (one line per sequence)    -src_dir   \nSource directory for image or audio files    -tgt   \nTrue target sequence (optional)    -output [pred.txt]  \nPath to output the predictions (each line will be the decoded sequence    -dynamic_dict   \nCreate dynamic dictionaries    -share_vocab   \nShare source and target vocabulary", 
            "title": "Data:"
        }, 
        {
            "location": "/options/translate/#beam", 
            "text": "-beam_size [5]  \nBeam size    -alpha   \nGoogle NMT length penalty parameter (higher = longer generation)    -beta   \nCoverage penalty parameter    -max_sent_length [100]  \nMaximum sentence length.    -replace_unk   \nReplace the generated UNK tokens with the source token that had highest\nattention weight. If phrase_table is provided, it will lookup the identified\nsource token and give the corresponding target token. If it is not provided(or\nthe identified source token does not exist in the table) then it will copy the\nsource token", 
            "title": "Beam:"
        }, 
        {
            "location": "/options/translate/#logging", 
            "text": "-verbose   \nPrint scores and predictions for each sentence    -attn_debug   \nPrint best attn for each word    -dump_beam   \nFile to dump beam information to.    -n_best [1]  \nIf verbose is set, will output the n_best decoded sentences", 
            "title": "Logging:"
        }, 
        {
            "location": "/options/translate/#efficiency", 
            "text": "-batch_size [30]  \nBatch size    -gpu [-1]  \nDevice to run on", 
            "title": "Efficiency:"
        }, 
        {
            "location": "/options/translate/#speech", 
            "text": "-sample_rate [16000]  \nSample rate.    -window_size [0.02]  \nWindow size for spectrogram in seconds    -window_stride [0.01]  \nWindow stride for spectrogram in seconds    -window [hamming]  \nWindow type for spectrogram generation", 
            "title": "Speech:"
        }, 
        {
            "location": "/CONTRIBUTORS/", 
            "text": "OpenNMT-py is a community developed project and we love developer contributions.\n\n\nBefore sending a PR, please do this checklist first:\n\n\n\n\nPlease run \ntools/pull_request_chk.sh\n and fix any errors. When adding new functionality, also add tests to this script. Included checks:\n\n\nflake8 check for coding style;\n\n\nunittest;\n\n\ncontinuous integration tests listed in \n.travis.yml\n.\n\n\n\n\n\n\nWhen adding/modifying class constructor, please make the arguments as same naming style as its superclass in pytorch.\n\n\nIf your change is based on a paper, please include a clear comment and reference in the code. \n\n\nIf your function takes/returns tensor arguments, please include assertions to document the sizes. See \nGlobalAttention.py\n for examples.", 
            "title": "Contributing"
        }
    ]
}